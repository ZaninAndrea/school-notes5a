---
category: "Matematica"
title: "Probabilità"
index: 2
draft: false
---

# Probabilità
La probabilità tratta **eventi aleatori**, due casi particolari sono gli eventi certi e gli eventi impossibili
$$
0 \le p(E) \le 1
$$
<!-- toc -->

## Probabilità classica (o a priori)
Quando a priori conosco tutta la gamma di eventi possibili. E.g. il lancio di un dado. 
$$
p(E) = \frac{\text{casi favorevoli}}{\text{casi possibili}}
$$
La probabilità dell'evento complementare è definita così:
$$
p(\overline{E}) = 1- p(E)
$$

### Con 2 eventi
$$
p(E_1 \cup E_2)= p(E_1) + p(E_2) - p (E_1 \cap E_2)
$$
Quando $$p(E_1 \cap E_2)=0$$ i 2 eventi sono incompatibili, altrimenti sono compatibili. 
$$
p(E_1 \cap E_2)= p(E_1) \cdot p(E_2)
$$


### Evento condizionato
Quando un evento precedente condiziona le probabilità di un evento successivo.
> Da una giara con 5 palline rosse, 2 nere e 3 blu estraggo 2 palline una dopo l'altra senza reinserirle nella giara, quali sono le probabilità di pescare uuna pallina blu e poi una rossa?
> $$
\begin{aligned}
p(BR) &= p(\text{B 1o estr}) \cdot p(\text{R 2o estr | è uscito blu al 1o}) \\
&= \frac{3}{10} \cdot \frac{5}{9}
\end{aligned}
$$

### Bayes
$$
p(A)\cdot p(B|A)=p(B)\cdot p(A|B)
$$

## Probabilità continua
$$
\begin{aligned}
f(x)&=P(X=x)\\
F(x)&=P(X\le x)
\end{aligned}
$$

$f$ è la distribuzione di probbilità, mentre $F$ è la ripartizione ed è l'integrale definito di $f$

Per definizione
$$
\int_{-\infty}^{+\infty}f(x)dx=1
$$

Il valore medio $M(X)$ è

$$
M(X)=\int_{-\infty}^{+\infty}xf(x)dx
$$

La varianza invece è
$$
Var(X)=\int_{-\infty}^{+\infty}\big[X-M(X\big]^2f(x)dx
$$

La deviazione standard invece è la radice della varianza.

### Varie distribuzioni
Distribuzione uniforme (e.g. dado):
$$
p(X=x_i)=\frac{1}{n}
$$

Distribuzione binomiale (e.g. moneta truccata)
$$
p(X=x_1)=\binom{n}{x} p^{n-x}(1-p)^x
$$

Distribuzione gaussiana (senza coefficienti di correzione per valore medio e varianza)
$$
P(X=x_1)\simeq e^{-x^2}
$$